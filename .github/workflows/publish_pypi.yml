name: PublishToPyPi

on:
  # On Release, Publish to PyPI
  workflow_dispatch:
    inputs:
      version:
        # Friendly description to be shown in the UI instead of 'version'
        description: New version in format n.n.n (example 0.1.5)
        # Default value if no value is explicitly provided
        default: 0.1.6
        # Input has to be provided for the workflow to run
        required: true

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  linting:
    name: "Run Pre-commit checks: black, flake8, isort, bandit etc.."
    runs-on: ubuntu-latest
    steps:
      #----------------------------------------------
      # Checkout, SetUp Python, Load Cache and Lint
      #----------------------------------------------
      - uses: actions/checkout@v2
      # Install Python
      - uses: actions/setup-python@v2
        with:
            python-version: 3.8
      - uses: actions/cache@v2
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip
          restore-keys: ${{ runner.os }}-pip
      - uses: pre-commit/action@v2.0.2
  build:
    name: "Build Deequ with Python ${{ matrix.python-version }} JDK ${{ matrix.java }}, Hadoop ${{ matrix.hadoop }}, Spark ${{ matrix.spark-version }} in ubuntu-20.04"
    # os: [ubuntu-latest, macos-latest, windows-latest]
    strategy:
      fail-fast: false
      matrix:
        include:
          - java: 8
            hadoop: 2.7
            spark-version: 2.4.7
            python-version: 3.6
          - java: 8
            hadoop: 3.2
            spark-version: 3.0.2
            python-version: 3.6
          - java: 11
            hadoop: 3.2
            spark-version: 3.0.2
            python-version: 3.6
          - java: 8
            hadoop: 2.7
            spark-version: 3.0.2
            python-version: 3.6
          - java: 11
            hadoop: 2.7
            spark-version: 3.0.2
            python-version: 3.6
          - java: 8
            hadoop: 2.7
            spark-version: 3.1.1
            python-version: 3.6
          - java: 11
            hadoop: 2.7
            spark-version: 3.1.1
            python-version: 3.6
          - java: 8
            hadoop: 3.2
            spark-version: 3.1.1
            python-version: 3.6
          - java: 11
            hadoop: 3.2
            spark-version: 3.1.1
            python-version: 3.6
          - java: 8
            hadoop: 3.2
            spark-version: 3.1.1
            python-version: 3.7
          - java: 11
            hadoop: 3.2
            spark-version: 3.1.1
            python-version: 3.7
          - java: 8
            hadoop: 3.2
            spark-version: 3.1.1
            python-version: 3.8
          - java: 11
            hadoop: 3.2
            spark-version: 3.1.1
            python-version: 3.8
    # Run on ubuntu
    runs-on: ubuntu-20.04
    env:
      GITHUB_PREV_SHA: ${{ github.event.before }}
      PYTHON_VERSION: ${{ matrix.python-version }}
      SPARK_VERSION: ${{ matrix.spark-version }}
      SCALA_VERSION: 2.12
    timeout-minutes: 60
    steps:
      - name: Checkout Spark repository
        uses: actions/checkout@v2
        with:
          fetch-depth: 0

      # Setup Python
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v2
        with:
          python-version: ${{ matrix.python-version }}

      - name: Set up JDK
        uses: actions/setup-java@v2
        with:
          java-version: ${{ matrix.java }}
          distribution: 'adopt'

      - name: Change Scala ${{ matrix.scala }} version
        run: |
          ls
          ./dev/change-scala-version.sh ${{ matrix.scala }}

      # Setup Python
      - name: Install Poetry
        uses: snok/install-poetry@v1.1.4
        with:
          virtualenvs-create: true
          virtualenvs-in-project: true

      # Install spark from apache-mirror: https://apachemirror.wuchna.com/spark/
      - name: Run setup-spark ${{ matrix.spark-version }}
        uses: vemonet/setup-spark@v1
        with:
          spark-version: ${{ matrix.spark-version }}
          hadoop-version: ${{ matrix.hadoop }}

      - name: Check Java ${{ matrix.java }} version
        run: java -version

      - name: Check Scala ${{ matrix.scala }} version
        run: scala -version

      - name: Check ${{ matrix.spark-version }} version
        run: spark-submit --version

      - name: Check Disk Size before Cache
        run: |
          df -hm

      # load cached venv if cache exists
      - name: Load cached venv
        id: cached-poetry-dependencies
        uses: actions/cache@v2
        with:
          path: .venv
          key: venv-${{ runner.os }}-${{ hashFiles('**/poetry.lock') }}

      # load cached maven if cache m2 exists
      - name: Cache Maven packages
        uses: actions/cache@v2
        with:
          path: ~/.m2
          key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}
          restore-keys: ${{ runner.os }}-m2

      # load cached maven if cache m2-com exists
      - name: Check Maven-com Cache
        uses: actions/cache@v2
        with:
          path: ~/.m2/repository/com
          key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            ${{ runner.os }}-m2-

      - name: Check Disk Size before Build
        run: |
          df -hm

      # Install dependencies if cache does not exist
      - name: Install poetry dependencies
        if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
        run: poetry add --no-interaction pyspark==${{ matrix.spark-version }} && poetry install --no-interaction --no-root
      - name: Install the package
        run: poetry install --no-interaction

      - name: Check the information about packages.
        run: poetry show

      - name: Check the version of the project
        run: poetry version

      # Run Tests
      # # python -m pytest -n auto --reruns 3 --reruns-delay 5
      - name: Run tests
        run: |
          # source .venv/bin/activate
          # python -m pytest
          poetry run pytest

      - name: Check Disk Size after Build
        run: |
          df -hm
